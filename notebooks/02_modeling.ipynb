{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ModÃ©lisation - SystÃ¨me de Recommandation MovieLens\n",
    "\n",
    "**Auteur:** Dady Akrou Cyrille  \n",
    "**Email:** cyrilledady0501@gmail.com  \n",
    "**Date:** DÃ©cembre 2024\n",
    "\n",
    "Ce notebook implÃ©mente et teste diffÃ©rents modÃ¨les de recommandation :\n",
    "- Filtrage collaboratif (SVD, NMF, KNN)\n",
    "- Filtrage par contenu (TF-IDF, Genres)\n",
    "- SystÃ¨mes hybrides (Weighted, Switching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports nÃ©cessaires\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Ajouter le rÃ©pertoire src au path\n",
    "sys.path.append(str(Path('../src').resolve()))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "import yaml\n",
    "import json\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Imports des modules personnalisÃ©s\n",
    "from data_processing.preprocess import MovieLensPreprocessor\n",
    "from models.collaborative_filtering import CollaborativeFilteringManager\n",
    "from models.content_based_filtering import ContentBasedManager\n",
    "from models.hybrid_system import HybridSystemManager\n",
    "from evaluation.metrics import ModelEvaluator\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Configuration des graphiques\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"âœ… Imports rÃ©alisÃ©s avec succÃ¨s!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement de la configuration\n",
    "config_path = Path('../config/config.yaml')\n",
    "\n",
    "with open(config_path, 'r', encoding='utf-8') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "print(\"Configuration chargÃ©e:\")\n",
    "print(f\"- Chemin des donnÃ©es: {config['data']['base_path']}\")\n",
    "print(f\"- ModÃ¨les Ã  tester: {len(config['models'])} types\")\n",
    "print(f\"- MÃ©triques d'Ã©valuation: {len(config['evaluation']['metrics'])} mÃ©triques\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PrÃ©paration des DonnÃ©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation du prÃ©processeur\n",
    "print(\"ğŸ”„ Initialisation du prÃ©processeur...\")\n",
    "preprocessor = MovieLensPreprocessor(config)\n",
    "\n",
    "# Chargement et prÃ©paration des donnÃ©es\n",
    "print(\"ğŸ“Š Chargement des donnÃ©es...\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Chargement\n",
    "    preprocessor.load_data()\n",
    "    print(f\"âœ… DonnÃ©es chargÃ©es: {len(preprocessor.ratings):,} ratings\")\n",
    "    \n",
    "    # Nettoyage\n",
    "    preprocessor.clean_data()\n",
    "    print(f\"âœ… DonnÃ©es nettoyÃ©es: {len(preprocessor.ratings):,} ratings\")\n",
    "    \n",
    "    # Traitement des films\n",
    "    preprocessor.process_movies()\n",
    "    print(f\"âœ… Films traitÃ©s: {len(preprocessor.movies):,} films\")\n",
    "    \n",
    "    # CrÃ©ation de la matrice utilisateur-film\n",
    "    preprocessor.create_user_movie_matrix()\n",
    "    print(f\"âœ… Matrice crÃ©Ã©e: {preprocessor.user_movie_matrix.shape}\")\n",
    "    \n",
    "    # Division des donnÃ©es\n",
    "    preprocessor.split_data()\n",
    "    print(f\"âœ… DonnÃ©es divisÃ©es:\")\n",
    "    print(f\"   - Train: {len(preprocessor.train_data):,} ratings\")\n",
    "    print(f\"   - Validation: {len(preprocessor.val_data):,} ratings\")\n",
    "    print(f\"   - Test: {len(preprocessor.test_data):,} ratings\")\n",
    "    \n",
    "    # Sauvegarde\n",
    "    preprocessor.save_processed_data()\n",
    "    print(\"âœ… DonnÃ©es sauvegardÃ©es\")\n",
    "    \n",
    "    processing_time = time.time() - start_time\n",
    "    print(f\"â±ï¸ Temps de traitement: {processing_time:.2f} secondes\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Erreur lors du prÃ©processing: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ModÃ¨les de Filtrage Collaboratif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation du gestionnaire de filtrage collaboratif\n",
    "print(\"ğŸ¤ === FILTRAGE COLLABORATIF ===")\n",
    "\n",
    "cf_manager = CollaborativeFilteringManager(config)\n",
    "\n",
    "# Chargement des donnÃ©es prÃ©processÃ©es\n",
    "cf_manager.load_data(\n",
    "    train_data=preprocessor.train_data,\n",
    "    val_data=preprocessor.val_data,\n",
    "    test_data=preprocessor.test_data\n",
    ")\n",
    "\n",
    "print(\"âœ… Gestionnaire de filtrage collaboratif initialisÃ©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EntraÃ®nement des modÃ¨les collaboratifs\n",
    "collaborative_results = {}\n",
    "\n",
    "models_to_train = ['svd', 'nmf', 'knn']\n",
    "\n",
    "for model_name in models_to_train:\n",
    "    print(f\"\\nğŸ”„ EntraÃ®nement du modÃ¨le {model_name.upper()}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # EntraÃ®nement\n",
    "        cf_manager.train_model(model_name)\n",
    "        \n",
    "        # Ã‰valuation\n",
    "        metrics = cf_manager.evaluate_model(model_name)\n",
    "        \n",
    "        # Sauvegarde\n",
    "        cf_manager.save_model(model_name)\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        collaborative_results[model_name] = {\n",
    "            'metrics': metrics,\n",
    "            'training_time': training_time\n",
    "        }\n",
    "        \n",
    "        print(f\"âœ… {model_name.upper()} entraÃ®nÃ© en {training_time:.2f}s\")\n",
    "        print(f\"   RMSE: {metrics.get('rmse', 'N/A'):.4f}\")\n",
    "        print(f\"   MAE: {metrics.get('mae', 'N/A'):.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erreur avec {model_name}: {e}\")\n",
    "        collaborative_results[model_name] = {'error': str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de recommandations collaboratives\n",
    "print(\"\\nğŸ¯ Test de recommandations collaboratives...\")\n",
    "\n",
    "# SÃ©lectionner un utilisateur de test\n",
    "test_user_id = preprocessor.test_data['userId'].iloc[0]\n",
    "print(f\"Utilisateur de test: {test_user_id}\")\n",
    "\n",
    "for model_name in models_to_train:\n",
    "    if model_name in collaborative_results and 'error' not in collaborative_results[model_name]:\n",
    "        try:\n",
    "            recommendations = cf_manager.get_recommendations(\n",
    "                model_name=model_name,\n",
    "                user_id=test_user_id,\n",
    "                n_recommendations=5\n",
    "            )\n",
    "            \n",
    "            print(f\"\\n{model_name.upper()} - Top 5 recommandations:\")\n",
    "            for i, (movie_id, score) in enumerate(recommendations, 1):\n",
    "                movie_title = preprocessor.movies[preprocessor.movies['movieId'] == movie_id]['title'].iloc[0]\n",
    "                print(f\"  {i}. {movie_title} (Score: {score:.3f})\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur recommandations {model_name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ModÃ¨les de Filtrage par Contenu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation du gestionnaire de filtrage par contenu\n",
    "print(\"ğŸ“ === FILTRAGE PAR CONTENU ===")\n",
    "\n",
    "content_manager = ContentBasedManager(config)\n",
    "\n",
    "# Chargement des donnÃ©es\n",
    "content_manager.load_data(\n",
    "    movies=preprocessor.movies,\n",
    "    ratings=preprocessor.ratings\n",
    ")\n",
    "\n",
    "print(\"âœ… Gestionnaire de filtrage par contenu initialisÃ©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EntraÃ®nement des modÃ¨les de contenu\n",
    "content_results = {}\n",
    "\n",
    "content_models = ['tfidf', 'genre_based']\n",
    "\n",
    "for model_name in content_models:\n",
    "    print(f\"\\nğŸ”„ EntraÃ®nement du modÃ¨le {model_name.upper()}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # EntraÃ®nement\n",
    "        content_manager.train_model(model_name)\n",
    "        \n",
    "        # Sauvegarde\n",
    "        content_manager.save_model(model_name)\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        content_results[model_name] = {\n",
    "            'training_time': training_time\n",
    "        }\n",
    "        \n",
    "        print(f\"âœ… {model_name.upper()} entraÃ®nÃ© en {training_time:.2f}s\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erreur avec {model_name}: {e}\")\n",
    "        content_results[model_name] = {'error': str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de recommandations par contenu\n",
    "print(\"\\nğŸ¯ Test de recommandations par contenu...\")\n",
    "\n",
    "# SÃ©lectionner un film de rÃ©fÃ©rence\n",
    "reference_movie_id = preprocessor.movies['movieId'].iloc[0]\n",
    "reference_movie_title = preprocessor.movies['title'].iloc[0]\n",
    "print(f\"Film de rÃ©fÃ©rence: {reference_movie_title}\")\n",
    "\n",
    "for model_name in content_models:\n",
    "    if model_name in content_results and 'error' not in content_results[model_name]:\n",
    "        try:\n",
    "            recommendations = content_manager.get_recommendations(\n",
    "                model_name=model_name,\n",
    "                movie_id=reference_movie_id,\n",
    "                n_recommendations=5\n",
    "            )\n",
    "            \n",
    "            print(f\"\\n{model_name.upper()} - Films similaires:\")\n",
    "            for i, (movie_id, score) in enumerate(recommendations, 1):\n",
    "                movie_title = preprocessor.movies[preprocessor.movies['movieId'] == movie_id]['title'].iloc[0]\n",
    "                print(f\"  {i}. {movie_title} (SimilaritÃ©: {score:.3f})\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur recommandations {model_name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. SystÃ¨mes Hybrides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation du gestionnaire hybride\n",
    "print(\"ğŸ”€ === SYSTÃˆMES HYBRIDES ===")\n",
    "\n",
    "hybrid_manager = HybridSystemManager(config)\n",
    "\n",
    "# Chargement des modÃ¨les prÃ©-entraÃ®nÃ©s\n",
    "hybrid_manager.load_models(\n",
    "    collaborative_manager=cf_manager,\n",
    "    content_manager=content_manager\n",
    ")\n",
    "\n",
    "print(\"âœ… Gestionnaire hybride initialisÃ©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EntraÃ®nement des modÃ¨les hybrides\n",
    "hybrid_results = {}\n",
    "\n",
    "hybrid_models = ['weighted', 'switching']\n",
    "\n",
    "for model_name in hybrid_models:\n",
    "    print(f\"\\nğŸ”„ EntraÃ®nement du modÃ¨le hybride {model_name.upper()}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # EntraÃ®nement\n",
    "        hybrid_manager.train_model(\n",
    "            model_name=model_name,\n",
    "            train_data=preprocessor.train_data,\n",
    "            val_data=preprocessor.val_data\n",
    "        )\n",
    "        \n",
    "        # Sauvegarde\n",
    "        hybrid_manager.save_model(model_name)\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        hybrid_results[model_name] = {\n",
    "            'training_time': training_time\n",
    "        }\n",
    "        \n",
    "        print(f\"âœ… {model_name.upper()} entraÃ®nÃ© en {training_time:.2f}s\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erreur avec {model_name}: {e}\")\n",
    "        hybrid_results[model_name] = {'error': str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de recommandations hybrides\n",
    "print(\"\\nğŸ¯ Test de recommandations hybrides...\")\n",
    "\n",
    "for model_name in hybrid_models:\n",
    "    if model_name in hybrid_results and 'error' not in hybrid_results[model_name]:\n",
    "        try:\n",
    "            recommendations = hybrid_manager.get_recommendations(\n",
    "                model_name=model_name,\n",
    "                user_id=test_user_id,\n",
    "                n_recommendations=5\n",
    "            )\n",
    "            \n",
    "            print(f\"\\n{model_name.upper()} - Top 5 recommandations hybrides:\")\n",
    "            for i, (movie_id, score) in enumerate(recommendations, 1):\n",
    "                movie_title = preprocessor.movies[preprocessor.movies['movieId'] == movie_id]['title'].iloc[0]\n",
    "                print(f\"  {i}. {movie_title} (Score: {score:.3f})\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur recommandations {model_name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Ã‰valuation Comparative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation de l'Ã©valuateur\n",
    "print(\"ğŸ“Š === Ã‰VALUATION COMPARATIVE ===")\n",
    "\n",
    "evaluator = ModelEvaluator(config)\n",
    "\n",
    "# Compilation des rÃ©sultats\n",
    "all_results = {}\n",
    "\n",
    "# Ajout des rÃ©sultats collaboratifs\n",
    "for model_name, results in collaborative_results.items():\n",
    "    if 'metrics' in results:\n",
    "        all_results[f'collaborative_{model_name}'] = results['metrics']\n",
    "\n",
    "# Ã‰valuation des modÃ¨les de contenu sur les donnÃ©es de test\n",
    "for model_name in content_models:\n",
    "    if model_name in content_results and 'error' not in content_results[model_name]:\n",
    "        try:\n",
    "            # Ã‰valuation simplifiÃ©e pour les modÃ¨les de contenu\n",
    "            content_metrics = evaluator.evaluate_content_model(\n",
    "                content_manager, model_name, preprocessor.test_data\n",
    "            )\n",
    "            all_results[f'content_{model_name}'] = content_metrics\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Impossible d'Ã©valuer {model_name}: {e}\")\n",
    "\n",
    "print(f\"âœ… {len(all_results)} modÃ¨les Ã©valuÃ©s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des rÃ©sultats comparatifs\n",
    "if all_results:\n",
    "    print(\"\\nğŸ“ˆ RÃ‰SULTATS COMPARATIFS:\")\n",
    "    print(\"=" * 60)\n",
    "    \n",
    "    # CrÃ©ation d'un DataFrame pour la comparaison\n",
    "    results_df = pd.DataFrame(all_results).T\n",
    "    \n",
    "    # Affichage des mÃ©triques principales\n",
    "    if 'rmse' in results_df.columns:\n",
    "        print(\"\\nğŸ¯ RMSE (plus bas = meilleur):\")\n",
    "        rmse_sorted = results_df['rmse'].dropna().sort_values()\n",
    "        for model, rmse in rmse_sorted.items():\n",
    "            print(f\"  {model:20s}: {rmse:.4f}\")\n",
    "    \n",
    "    if 'mae' in results_df.columns:\n",
    "        print(\"\\nğŸ¯ MAE (plus bas = meilleur):\")\n",
    "        mae_sorted = results_df['mae'].dropna().sort_values()\n",
    "        for model, mae in mae_sorted.items():\n",
    "            print(f\"  {model:20s}: {mae:.4f}\")\n",
    "    \n",
    "    # Affichage du tableau complet\n",
    "    print(\"\\nğŸ“Š TABLEAU COMPLET DES RÃ‰SULTATS:\")\n",
    "    display(results_df.round(4))\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸ Aucun rÃ©sultat disponible pour la comparaison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des rÃ©sultats\n",
    "if all_results and len(all_results) > 1:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Graphique RMSE\n",
    "    if 'rmse' in results_df.columns:\n",
    "        rmse_data = results_df['rmse'].dropna()\n",
    "        if len(rmse_data) > 0:\n",
    "            rmse_data.plot(kind='bar', ax=axes[0], color='skyblue', edgecolor='black')\n",
    "            axes[0].set_title('Comparaison RMSE par ModÃ¨le')\n",
    "            axes[0].set_ylabel('RMSE')\n",
    "            axes[0].tick_params(axis='x', rotation=45)\n",
    "            axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Graphique MAE\n",
    "    if 'mae' in results_df.columns:\n",
    "        mae_data = results_df['mae'].dropna()\n",
    "        if len(mae_data) > 0:\n",
    "            mae_data.plot(kind='bar', ax=axes[1], color='lightcoral', edgecolor='black')\n",
    "            axes[1].set_title('Comparaison MAE par ModÃ¨le')\n",
    "            axes[1].set_ylabel('MAE')\n",
    "            axes[1].tick_params(axis='x', rotation=45)\n",
    "            axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Graphique radar pour comparaison multi-mÃ©triques\n",
    "    if len(results_df.columns) >= 3:\n",
    "        try:\n",
    "            # Normalisation des mÃ©triques pour le graphique radar\n",
    "            normalized_df = results_df.copy()\n",
    "            for col in normalized_df.columns:\n",
    "                if col in ['rmse', 'mae']:  # MÃ©triques Ã  minimiser\n",
    "                    normalized_df[col] = 1 - (normalized_df[col] / normalized_df[col].max())\n",
    "                else:  # MÃ©triques Ã  maximiser\n",
    "                    normalized_df[col] = normalized_df[col] / normalized_df[col].max()\n",
    "            \n",
    "            # CrÃ©ation du graphique radar avec plotly\n",
    "            fig_radar = go.Figure()\n",
    "            \n",
    "            for model in normalized_df.index:\n",
    "                fig_radar.add_trace(go.Scatterpolar(\n",
    "                    r=normalized_df.loc[model].values,\n",
    "                    theta=normalized_df.columns,\n",
    "                    fill='toself',\n",
    "                    name=model\n",
    "                ))\n",
    "            \n",
    "            fig_radar.update_layout(\n",
    "                polar=dict(\n",
    "                    radialaxis=dict(\n",
    "                        visible=True,\n",
    "                        range=[0, 1]\n",
    "                    )),\n",
    "                showlegend=True,\n",
    "                title=\"Comparaison Multi-MÃ©triques des ModÃ¨les\"\n",
    "            )\n",
    "            \n",
    "            fig_radar.show()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Impossible de crÃ©er le graphique radar: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyse des Temps d'ExÃ©cution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilation des temps d'entraÃ®nement\n",
    "training_times = {}\n",
    "\n",
    "# Temps collaboratifs\n",
    "for model_name, results in collaborative_results.items():\n",
    "    if 'training_time' in results:\n",
    "        training_times[f'collaborative_{model_name}'] = results['training_time']\n",
    "\n",
    "# Temps de contenu\n",
    "for model_name, results in content_results.items():\n",
    "    if 'training_time' in results:\n",
    "        training_times[f'content_{model_name}'] = results['training_time']\n",
    "\n",
    "# Temps hybrides\n",
    "for model_name, results in hybrid_results.items():\n",
    "    if 'training_time' in results:\n",
    "        training_times[f'hybrid_{model_name}'] = results['training_time']\n",
    "\n",
    "if training_times:\n",
    "    print(\"â±ï¸ TEMPS D'ENTRAÃNEMENT:\")\n",
    "    print(\"=" * 40)\n",
    "    \n",
    "    # Tri par temps\n",
    "    sorted_times = sorted(training_times.items(), key=lambda x: x[1])\n",
    "    \n",
    "    for model, time_sec in sorted_times:\n",
    "        print(f\"  {model:25s}: {time_sec:6.2f}s\")\n",
    "    \n",
    "    # Visualisation\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    models = [item[0] for item in sorted_times]\n",
    "    times = [item[1] for item in sorted_times]\n",
    "    \n",
    "    bars = plt.bar(models, times, color='lightgreen', edgecolor='black')\n",
    "    plt.title('Temps d\\'EntraÃ®nement par ModÃ¨le')\n",
    "    plt.ylabel('Temps (secondes)')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Ajout des valeurs sur les barres\n",
    "    for bar, time_val in zip(bars, times):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "                f'{time_val:.1f}s', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Recommandations et Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse des rÃ©sultats et recommandations\n",
    "print(\"ğŸ¯ === ANALYSE ET RECOMMANDATIONS ===")\n",
    "print()\n",
    "\n",
    "# Identification du meilleur modÃ¨le\n",
    "if all_results:\n",
    "    if 'rmse' in results_df.columns:\n",
    "        best_rmse_model = results_df['rmse'].dropna().idxmin()\n",
    "        best_rmse_value = results_df.loc[best_rmse_model, 'rmse']\n",
    "        print(f\"ğŸ† MEILLEUR MODÃˆLE (RMSE): {best_rmse_model}\")\n",
    "        print(f\"   RMSE: {best_rmse_value:.4f}\")\n",
    "    \n",
    "    if 'mae' in results_df.columns:\n",
    "        best_mae_model = results_df['mae'].dropna().idxmin()\n",
    "        best_mae_value = results_df.loc[best_mae_model, 'mae']\n",
    "        print(f\"ğŸ† MEILLEUR MODÃˆLE (MAE): {best_mae_model}\")\n",
    "        print(f\"   MAE: {best_mae_value:.4f}\")\n",
    "\n",
    "print(\"\\nğŸ“‹ RECOMMANDATIONS POUR LA PRODUCTION:\")\n",
    "print(\"\\n1. ğŸš€ DÃ‰PLOIEMENT:\")\n",
    "print(\"   â€¢ Utiliser le modÃ¨le avec le meilleur RMSE pour la prÃ©diction de ratings\")\n",
    "print(\"   â€¢ ImplÃ©menter un systÃ¨me hybride pour combiner les approches\")\n",
    "print(\"   â€¢ Mettre en place un cache pour les recommandations frÃ©quentes\")\n",
    "\n",
    "print(\"\\n2. ğŸ”„ AMÃ‰LIORATION CONTINUE:\")\n",
    "print(\"   â€¢ RÃ©entraÃ®ner les modÃ¨les pÃ©riodiquement avec de nouvelles donnÃ©es\")\n",
    "print(\"   â€¢ Monitorer les performances en temps rÃ©el\")\n",
    "print(\"   â€¢ Collecter les feedbacks utilisateurs pour l'amÃ©lioration\")\n",
    "\n",
    "print(\"\\n3. ğŸ“Š MÃ‰TRIQUES DE SUIVI:\")\n",
    "print(\"   â€¢ RMSE/MAE pour la prÃ©cision des prÃ©dictions\")\n",
    "print(\"   â€¢ Taux de clic sur les recommandations\")\n",
    "print(\"   â€¢ DiversitÃ© et nouveautÃ© des recommandations\")\n",
    "print(\"   â€¢ Temps de rÃ©ponse de l'API\")\n",
    "\n",
    "print(\"\\n4. ğŸ›¡ï¸ GESTION DES PROBLÃˆMES:\")\n",
    "print(\"   â€¢ Cold start: Recommandations basÃ©es sur la popularitÃ©\")\n",
    "print(\"   â€¢ SparsitÃ©: Techniques de rÃ©gularisation\")\n",
    "print(\"   â€¢ ScalabilitÃ©: Optimisation des algorithmes et infrastructure\")\n",
    "\n",
    "print(\"\\n5. ğŸ¨ EXPÃ‰RIENCE UTILISATEUR:\")\n",
    "print(\"   â€¢ Interface intuitive avec explications des recommandations\")\n",
    "print(\"   â€¢ Personnalisation basÃ©e sur l'historique\")\n",
    "print(\"   â€¢ Options de filtrage par genre, annÃ©e, etc.\")\n",
    "print(\"   â€¢ SystÃ¨me de feedback pour amÃ©liorer les recommandations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Sauvegarde des RÃ©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde complÃ¨te des rÃ©sultats\n",
    "modeling_results = {\n",
    "    'experiment_info': {\n",
    "        'date': str(datetime.now()),\n",
    "        'dataset_size': len(preprocessor.ratings) if hasattr(preprocessor, 'ratings') else 0,\n",
    "        'train_size': len(preprocessor.train_data) if hasattr(preprocessor, 'train_data') else 0,\n",
    "        'test_size': len(preprocessor.test_data) if hasattr(preprocessor, 'test_data') else 0\n",
    "    },\n",
    "    'collaborative_results': collaborative_results,\n",
    "    'content_results': content_results,\n",
    "    'hybrid_results': hybrid_results,\n",
    "    'training_times': training_times,\n",
    "    'evaluation_metrics': all_results,\n",
    "    'best_models': {}\n",
    "}\n",
    "\n",
    "# Identification des meilleurs modÃ¨les\n",
    "if all_results and len(all_results) > 0:\n",
    "    if 'rmse' in results_df.columns and not results_df['rmse'].dropna().empty:\n",
    "        modeling_results['best_models']['rmse'] = {\n",
    "            'model': results_df['rmse'].dropna().idxmin(),\n",
    "            'value': float(results_df['rmse'].dropna().min())\n",
    "        }\n",
    "    \n",
    "    if 'mae' in results_df.columns and not results_df['mae'].dropna().empty:\n",
    "        modeling_results['best_models']['mae'] = {\n",
    "            'model': results_df['mae'].dropna().idxmin(),\n",
    "            'value': float(results_df['mae'].dropna().min())\n",
    "        }\n",
    "\n",
    "# Sauvegarde\n",
    "results_path = Path('../data/processed/modeling_results.json')\n",
    "results_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(results_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(modeling_results, f, indent=2, ensure_ascii=False, default=str)\n",
    "\n",
    "print(\"âœ… RÃ©sultats sauvegardÃ©s dans data/processed/modeling_results.json\")\n",
    "\n",
    "# RÃ©sumÃ© final\n",
    "print(\"\\nğŸ‰ === RÃ‰SUMÃ‰ DE LA MODÃ‰LISATION ===")\n",
    "print(f\"ğŸ“Š ModÃ¨les entraÃ®nÃ©s: {len(collaborative_results) + len(content_results) + len(hybrid_results)}\")\n",
    "print(f\"â±ï¸ Temps total: {sum(training_times.values()):.2f} secondes\")\n",
    "print(f\"ğŸ“ˆ MÃ©triques calculÃ©es: {len(all_results)} modÃ¨les Ã©valuÃ©s\")\n",
    "print(\"ğŸš€ PrÃªt pour le dÃ©ploiement!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}