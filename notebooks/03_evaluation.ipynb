{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Évaluation Approfondie - Système de Recommandation MovieLens\n",
    "\n",
    "**Auteur:** Dady Akrou Cyrille  \n",
    "**Email:** cyrilledady0501@gmail.com  \n",
    "**Date:** Décembre 2024\n",
    "\n",
    "Ce notebook effectue une évaluation approfondie des modèles de recommandation :\n",
    "- Métriques de précision (RMSE, MAE)\n",
    "- Métriques de ranking (Precision@K, Recall@K, NDCG@K)\n",
    "- Métriques de diversité et de couverture\n",
    "- Analyse des biais et de la robustesse\n",
    "- Tests A/B simulés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports nécessaires\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Ajouter le répertoire src au path\n",
    "sys.path.append(str(Path('../src').resolve()))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "import yaml\n",
    "import json\n",
    "from datetime import datetime\n",
    "import time\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "# Imports des modules personnalisés\n",
    "from data_processing.preprocess import MovieLensPreprocessor\n",
    "from models.collaborative_filtering import CollaborativeFilteringManager\n",
    "from models.content_based_filtering import ContentBasedManager\n",
    "from models.hybrid_system import HybridSystemManager\n",
    "from evaluation.metrics import ModelEvaluator, RecommendationMetrics\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Configuration des graphiques\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"✅ Imports réalisés avec succès!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement de la configuration\n",
    "config_path = Path('../config/config.yaml')\n",
    "\n",
    "with open(config_path, 'r', encoding='utf-8') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Chargement des résultats de modélisation précédents\n",
    "results_path = Path('../data/processed/modeling_results.json')\n",
    "\n",
    "if results_path.exists():\n",
    "    with open(results_path, 'r', encoding='utf-8') as f:\n",
    "        previous_results = json.load(f)\n",
    "    print(\"✅ Résultats de modélisation chargés\")\n",
    "else:\n",
    "    previous_results = None\n",
    "    print(\"⚠️ Aucun résultat de modélisation trouvé\")\n",
    "\n",
    "print(f\"Configuration chargée: {len(config['evaluation']['metrics'])} métriques d'évaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chargement des Données et Modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation du préprocesseur et chargement des données\n",
    "print(\"🔄 Chargement des données préprocessées...\")\n",
    "\n",
    "preprocessor = MovieLensPreprocessor(config)\n",
    "\n",
    "try:\n",
    "    # Chargement des données sauvegardées\n",
    "    processed_data_path = Path(config['data']['processed_path'])\n",
    "    \n",
    "    if (processed_data_path / 'train_data.csv').exists():\n",
    "        preprocessor.train_data = pd.read_csv(processed_data_path / 'train_data.csv')\n",
    "        preprocessor.val_data = pd.read_csv(processed_data_path / 'val_data.csv')\n",
    "        preprocessor.test_data = pd.read_csv(processed_data_path / 'test_data.csv')\n",
    "        preprocessor.movies = pd.read_csv(processed_data_path / 'movies_processed.csv')\n",
    "        \n",
    "        print(f\"✅ Données chargées:\")\n",
    "        print(f\"   - Train: {len(preprocessor.train_data):,} ratings\")\n",
    "        print(f\"   - Validation: {len(preprocessor.val_data):,} ratings\")\n",
    "        print(f\"   - Test: {len(preprocessor.test_data):,} ratings\")\n",
    "        print(f\"   - Films: {len(preprocessor.movies):,} films\")\n",
    "    else:\n",
    "        print(\"⚠️ Données préprocessées non trouvées, rechargement nécessaire\")\n",
    "        # Recharger et préprocesser\n",
    "        preprocessor.load_data()\n",
    "        preprocessor.clean_data()\n",
    "        preprocessor.process_movies()\n",
    "        preprocessor.split_data()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Erreur lors du chargement: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation des gestionnaires de modèles\n",
    "print(\"🤖 Initialisation des gestionnaires de modèles...\")\n",
    "\n",
    "# Gestionnaire collaboratif\n",
    "cf_manager = CollaborativeFilteringManager(config)\n",
    "cf_manager.load_data(\n",
    "    train_data=preprocessor.train_data,\n",
    "    val_data=preprocessor.val_data,\n",
    "    test_data=preprocessor.test_data\n",
    ")\n",
    "\n",
    "# Gestionnaire de contenu\n",
    "content_manager = ContentBasedManager(config)\n",
    "content_manager.load_data(\n",
    "    movies=preprocessor.movies,\n",
    "    ratings=preprocessor.train_data\n",
    ")\n",
    "\n",
    "# Gestionnaire hybride\n",
    "hybrid_manager = HybridSystemManager(config)\n",
    "\n",
    "# Évaluateur\n",
    "evaluator = ModelEvaluator(config)\n",
    "metrics_calculator = RecommendationMetrics()\n",
    "\n",
    "print(\"✅ Gestionnaires initialisés\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chargement des Modèles Pré-entraînés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des modèles sauvegardés\n",
    "print(\"📂 Chargement des modèles pré-entraînés...\")\n",
    "\n",
    "models_path = Path(config['data']['models_path'])\n",
    "loaded_models = {}\n",
    "\n",
    "# Modèles collaboratifs\n",
    "collaborative_models = ['svd', 'nmf', 'knn']\n",
    "for model_name in collaborative_models:\n",
    "    try:\n",
    "        cf_manager.load_model(model_name)\n",
    "        loaded_models[f'collaborative_{model_name}'] = cf_manager\n",
    "        print(f\"✅ Modèle {model_name.upper()} chargé\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Impossible de charger {model_name}: {e}\")\n",
    "\n",
    "# Modèles de contenu\n",
    "content_models = ['tfidf', 'genre_based']\n",
    "for model_name in content_models:\n",
    "    try:\n",
    "        content_manager.load_model(model_name)\n",
    "        loaded_models[f'content_{model_name}'] = content_manager\n",
    "        print(f\"✅ Modèle {model_name.upper()} chargé\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Impossible de charger {model_name}: {e}\")\n",
    "\n",
    "# Modèles hybrides\n",
    "hybrid_models = ['weighted', 'switching']\n",
    "for model_name in hybrid_models:\n",
    "    try:\n",
    "        hybrid_manager.load_model(model_name)\n",
    "        loaded_models[f'hybrid_{model_name}'] = hybrid_manager\n",
    "        print(f\"✅ Modèle hybride {model_name.upper()} chargé\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Impossible de charger {model_name}: {e}\")\n",
    "\n",
    "print(f\"\\n📊 Total: {len(loaded_models)} modèles chargés pour l'évaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Évaluation Complète des Modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction d'évaluation complète\n",
    "def comprehensive_evaluation(model_manager, model_name, test_data, movies_data, k_values=[5, 10, 20]):\n",
    "    \"\"\"Évaluation complète d'un modèle\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    print(f\"🔍 Évaluation de {model_name}...\")\n",
    "    \n",
    "    try:\n",
    "        # 1. Métriques de précision (pour les modèles collaboratifs)\n",
    "        if 'collaborative' in model_name:\n",
    "            precision_metrics = model_manager.evaluate_model(model_name.split('_')[1])\n",
    "            results.update(precision_metrics)\n",
    "        \n",
    "        # 2. Métriques de ranking\n",
    "        ranking_metrics = {}\n",
    "        \n",
    "        # Échantillonnage pour accélérer l'évaluation\n",
    "        sample_users = test_data['userId'].unique()[:100]  # 100 utilisateurs\n",
    "        \n",
    "        all_precisions = {k: [] for k in k_values}\n",
    "        all_recalls = {k: [] for k in k_values}\n",
    "        all_ndcgs = {k: [] for k in k_values}\n",
    "        \n",
    "        for user_id in sample_users:\n",
    "            # Films réellement appréciés par l'utilisateur (rating >= 4)\n",
    "            user_test_data = test_data[test_data['userId'] == user_id]\n",
    "            relevant_items = set(user_test_data[user_test_data['rating'] >= 4]['movieId'].tolist())\n",
    "            \n",
    "            if len(relevant_items) == 0:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Obtenir les recommandations\n",
    "                if 'collaborative' in model_name:\n",
    "                    recommendations = model_manager.get_recommendations(\n",
    "                        model_name.split('_')[1], user_id, max(k_values)\n",
    "                    )\n",
    "                elif 'content' in model_name:\n",
    "                    # Pour les modèles de contenu, utiliser un film de l'historique\n",
    "                    user_history = preprocessor.train_data[preprocessor.train_data['userId'] == user_id]\n",
    "                    if len(user_history) > 0:\n",
    "                        ref_movie = user_history.iloc[0]['movieId']\n",
    "                        recommendations = model_manager.get_recommendations(\n",
    "                            model_name.split('_')[1], ref_movie, max(k_values)\n",
    "                        )\n",
    "                    else:\n",
    "                        continue\n",
    "                elif 'hybrid' in model_name:\n",
    "                    recommendations = model_manager.get_recommendations(\n",
    "                        model_name.split('_')[1], user_id, max(k_values)\n",
    "                    )\n",
    "                \n",
    "                recommended_items = [item[0] for item in recommendations]\n",
    "                \n",
    "                # Calculer les métriques pour chaque k\n",
    "                for k in k_values:\n",
    "                    top_k_items = set(recommended_items[:k])\n",
    "                    \n",
    "                    # Precision@k\n",
    "                    precision_k = len(top_k_items.intersection(relevant_items)) / k\n",
    "                    all_precisions[k].append(precision_k)\n",
    "                    \n",
    "                    # Recall@k\n",
    "                    recall_k = len(top_k_items.intersection(relevant_items)) / len(relevant_items)\n",
    "                    all_recalls[k].append(recall_k)\n",
    "                    \n",
    "                    # NDCG@k (simplifié)\n",
    "                    dcg = 0\n",
    "                    for i, item in enumerate(recommended_items[:k]):\n",
    "                        if item in relevant_items:\n",
    "                            dcg += 1 / np.log2(i + 2)\n",
    "                    \n",
    "                    # IDCG (ideal DCG)\n",
    "                    idcg = sum(1 / np.log2(i + 2) for i in range(min(k, len(relevant_items))))\n",
    "                    \n",
    "                    ndcg_k = dcg / idcg if idcg > 0 else 0\n",
    "                    all_ndcgs[k].append(ndcg_k)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        # Moyennes des métriques\n",
    "        for k in k_values:\n",
    "            if all_precisions[k]:\n",
    "                ranking_metrics[f'precision_at_{k}'] = np.mean(all_precisions[k])\n",
    "                ranking_metrics[f'recall_at_{k}'] = np.mean(all_recalls[k])\n",
    "                ranking_metrics[f'ndcg_at_{k}'] = np.mean(all_ndcgs[k])\n",
    "                ranking_metrics[f'f1_at_{k}'] = 2 * ranking_metrics[f'precision_at_{k}'] * ranking_metrics[f'recall_at_{k}'] / (ranking_metrics[f'precision_at_{k}'] + ranking_metrics[f'recall_at_{k}']) if (ranking_metrics[f'precision_at_{k}'] + ranking_metrics[f'recall_at_{k}']) > 0 else 0\n",
    "        \n",
    "        results.update(ranking_metrics)\n",
    "        \n",
    "        # 3. Métriques de diversité\n",
    "        diversity_metrics = calculate_diversity_metrics(model_manager, model_name, sample_users[:20], movies_data)\n",
    "        results.update(diversity_metrics)\n",
    "        \n",
    "        print(f\"✅ {model_name} évalué\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur lors de l'évaluation de {model_name}: {e}\")\n",
    "        results['error'] = str(e)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def calculate_diversity_metrics(model_manager, model_name, sample_users, movies_data):\n",
    "    \"\"\"Calcul des métriques de diversité\"\"\"\n",
    "    diversity_metrics = {}\n",
    "    \n",
    "    try:\n",
    "        all_recommendations = []\n",
    "        all_genres = []\n",
    "        \n",
    "        for user_id in sample_users:\n",
    "            try:\n",
    "                if 'collaborative' in model_name:\n",
    "                    recommendations = model_manager.get_recommendations(\n",
    "                        model_name.split('_')[1], user_id, 10\n",
    "                    )\n",
    "                elif 'content' in model_name:\n",
    "                    user_history = preprocessor.train_data[preprocessor.train_data['userId'] == user_id]\n",
    "                    if len(user_history) > 0:\n",
    "                        ref_movie = user_history.iloc[0]['movieId']\n",
    "                        recommendations = model_manager.get_recommendations(\n",
    "                            model_name.split('_')[1], ref_movie, 10\n",
    "                        )\n",
    "                    else:\n",
    "                        continue\n",
    "                elif 'hybrid' in model_name:\n",
    "                    recommendations = model_manager.get_recommendations(\n",
    "                        model_name.split('_')[1], user_id, 10\n",
    "                    )\n",
    "                \n",
    "                recommended_movies = [item[0] for item in recommendations]\n",
    "                all_recommendations.extend(recommended_movies)\n",
    "                \n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        if all_recommendations:\n",
    "            # Coverage: pourcentage de films uniques recommandés\n",
    "            unique_recommendations = len(set(all_recommendations))\n",
    "            total_movies = len(movies_data)\n",
    "            diversity_metrics['coverage'] = unique_recommendations / total_movies\n",
    "            \n",
    "            # Diversité des genres\n",
    "            if all_genres:\n",
    "                unique_genres = len(set(all_genres))\n",
    "                total_genres = len(set([genre for genres in movies_data['genres'].dropna() for genre in genres.split('|')]))\n",
    "                diversity_metrics['genre_diversity'] = unique_genres / total_genres\n",
    "                \n",
    "                # Entropie des genres\n",
    "                genre_counts = pd.Series(all_genres).value_counts()\n",
    "                genre_probs = genre_counts / genre_counts.sum()\n",
    "                diversity_metrics['genre_entropy'] = -sum(p * np.log2(p) for p in genre_probs if p > 0)\n",
    "        \n",
    "    except Exception as e:\n",
    "        diversity_metrics['diversity_error'] = str(e)\n",
    "    \n",
    "    return diversity_metrics\n",
    "\n",
    "print(\"✅ Fonctions d'évaluation définies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluation complète de tous les modèles\n",
    "print(\"🚀 === ÉVALUATION COMPLÈTE DES MODÈLES ===")\n",
    "\n",
    "comprehensive_results = {}\n",
    "evaluation_start_time = time.time()\n",
    "\n",
    "# Liste des modèles à évaluer\n",
    "models_to_evaluate = []\n",
    "\n",
    "# Ajouter les modèles collaboratifs\n",
    "for model in collaborative_models:\n",
    "    if f'collaborative_{model}' in loaded_models:\n",
    "        models_to_evaluate.append((f'collaborative_{model}', cf_manager))\n",
    "\n",
    "# Ajouter les modèles de contenu\n",
    "for model in content_models:\n",
    "    if f'content_{model}' in loaded_models:\n",
    "        models_to_evaluate.append((f'content_{model}', content_manager))\n",
    "\n",
    "# Ajouter les modèles hybrides\n",
    "for model in hybrid_models:\n",
    "    if f'hybrid_{model}' in loaded_models:\n",
    "        models_to_evaluate.append((f'hybrid_{model}', hybrid_manager))\n",
    "\n",
    "print(f\"📊 {len(models_to_evaluate)} modèles à évaluer\")\n",
    "\n",
    "# Évaluation de chaque modèle\n",
    "for model_name, model_manager in models_to_evaluate:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    results = comprehensive_evaluation(\n",
    "        model_manager, \n",
    "        model_name, \n",
    "        preprocessor.test_data, \n",
    "        preprocessor.movies\n",
    "    )\n",
    "    \n",
    "    evaluation_time = time.time() - start_time\n",
    "    results['evaluation_time'] = evaluation_time\n",
    "    \n",
    "    comprehensive_results[model_name] = results\n",
    "    \n",
    "    print(f\"⏱️ {model_name} évalué en {evaluation_time:.2f}s\")\n",
    "\n",
    "total_evaluation_time = time.time() - evaluation_start_time\n",
    "print(f\"\\n✅ Évaluation complète terminée en {total_evaluation_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyse Comparative Détaillée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du DataFrame de résultats\n",
    "if comprehensive_results:\n",
    "    results_df = pd.DataFrame(comprehensive_results).T\n",
    "    \n",
    "    print(\"📊 === RÉSULTATS COMPARATIFS DÉTAILLÉS ===")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Affichage des métriques principales\n",
    "    print(\"\\n🎯 MÉTRIQUES DE PRÉCISION:\")\n",
    "    precision_cols = [col for col in results_df.columns if 'rmse' in col or 'mae' in col]\n",
    "    if precision_cols:\n",
    "        precision_df = results_df[precision_cols].dropna()\n",
    "        if not precision_df.empty:\n",
    "            display(precision_df.round(4))\n",
    "    \n",
    "    print(\"\\n🎯 MÉTRIQUES DE RANKING (K=10):\")\n",
    "    ranking_cols = [col for col in results_df.columns if '_at_10' in col]\n",
    "    if ranking_cols:\n",
    "        ranking_df = results_df[ranking_cols].dropna()\n",
    "        if not ranking_df.empty:\n",
    "            display(ranking_df.round(4))\n",
    "    \n",
    "    print(\"\\n🎯 MÉTRIQUES DE DIVERSITÉ:\")\n",
    "    diversity_cols = [col for col in results_df.columns if 'coverage' in col or 'diversity' in col or 'entropy' in col]\n",
    "    if diversity_cols:\n",
    "        diversity_df = results_df[diversity_cols].dropna()\n",
    "        if not diversity_df.empty:\n",
    "            display(diversity_df.round(4))\n",
    "    \n",
    "    # Tableau complet\n",
    "    print(\"\\n📋 TABLEAU COMPLET:\")\n",
    "    display(results_df.round(4))\n",
    "\n",
    "else:\n",
    "    print(\"⚠️ Aucun résultat d'évaluation disponible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identification des meilleurs modèles par catégorie\n",
    "if comprehensive_results and not results_df.empty:\n",
    "    print(\"🏆 === CLASSEMENT DES MODÈLES ===")\n",
    "    print()\n",
    "    \n",
    "    best_models = {}\n",
    "    \n",
    "    # Meilleur pour la précision (RMSE)\n",
    "    if 'rmse' in results_df.columns:\n",
    "        rmse_data = results_df['rmse'].dropna()\n",
    "        if not rmse_data.empty:\n",
    "            best_rmse = rmse_data.idxmin()\n",
    "            best_models['precision_rmse'] = (best_rmse, rmse_data[best_rmse])\n",
    "            print(f\"🥇 MEILLEUR RMSE: {best_rmse} ({rmse_data[best_rmse]:.4f})\")\n",
    "    \n",
    "    # Meilleur pour MAE\n",
    "    if 'mae' in results_df.columns:\n",
    "        mae_data = results_df['mae'].dropna()\n",
    "        if not mae_data.empty:\n",
    "            best_mae = mae_data.idxmin()\n",
    "            best_models['precision_mae'] = (best_mae, mae_data[best_mae])\n",
    "            print(f\"🥇 MEILLEUR MAE: {best_mae} ({mae_data[best_mae]:.4f})\")\n",
    "    \n",
    "    # Meilleur pour Precision@10\n",
    "    if 'precision_at_10' in results_df.columns:\n",
    "        prec10_data = results_df['precision_at_10'].dropna()\n",
    "        if not prec10_data.empty:\n",
    "            best_prec10 = prec10_data.idxmax()\n",
    "            best_models['ranking_precision'] = (best_prec10, prec10_data[best_prec10])\n",
    "            print(f\"🥇 MEILLEUR PRECISION@10: {best_prec10} ({prec10_data[best_prec10]:.4f})\")\n",
    "    \n",
    "    # Meilleur pour NDCG@10\n",
    "    if 'ndcg_at_10' in results_df.columns:\n",
    "        ndcg10_data = results_df['ndcg_at_10'].dropna()\n",
    "        if not ndcg10_data.empty:\n",
    "            best_ndcg10 = ndcg10_data.idxmax()\n",
    "            best_models['ranking_ndcg'] = (best_ndcg10, ndcg10_data[best_ndcg10])\n",
    "            print(f\"🥇 MEILLEUR NDCG@10: {best_ndcg10} ({ndcg10_data[best_ndcg10]:.4f})\")\n",
    "    \n",
    "    # Meilleur pour la diversité\n",
    "    if 'coverage' in results_df.columns:\n",
    "        coverage_data = results_df['coverage'].dropna()\n",
    "        if not coverage_data.empty:\n",
    "            best_coverage = coverage_data.idxmax()\n",
    "            best_models['diversity_coverage'] = (best_coverage, coverage_data[best_coverage])\n",
    "            print(f\"🥇 MEILLEURE COUVERTURE: {best_coverage} ({coverage_data[best_coverage]:.4f})\")\n",
    "    \n",
    "    # Temps d'évaluation\n",
    "    if 'evaluation_time' in results_df.columns:\n",
    "        time_data = results_df['evaluation_time'].dropna()\n",
    "        if not time_data.empty:\n",
    "            fastest_model = time_data.idxmin()\n",
    "            print(f\"⚡ PLUS RAPIDE: {fastest_model} ({time_data[fastest_model]:.2f}s)\")\n",
    "    \n",
    "    # Score composite (moyenne normalisée)\n",
    "    print(\"\\n🎯 SCORE COMPOSITE (moyenne normalisée):\")\n",
    "    composite_scores = {}\n",
    "    \n",
    "    for model in results_df.index:\n",
    "        score = 0\n",
    "        count = 0\n",
    "        \n",
    "        # RMSE (normalisé inversé)\n",
    "        if 'rmse' in results_df.columns and pd.notna(results_df.loc[model, 'rmse']):\n",
    "            rmse_norm = 1 - (results_df.loc[model, 'rmse'] - results_df['rmse'].min()) / (results_df['rmse'].max() - results_df['rmse'].min())\n",
    "            score += rmse_norm * 0.3\n",
    "            count += 0.3\n",
    "        \n",
    "        # Precision@10\n",
    "        if 'precision_at_10' in results_df.columns and pd.notna(results_df.loc[model, 'precision_at_10']):\n",
    "            prec_norm = (results_df.loc[model, 'precision_at_10'] - results_df['precision_at_10'].min()) / (results_df['precision_at_10'].max() - results_df['precision_at_10'].min())\n",
    "            score += prec_norm * 0.3\n",
    "            count += 0.3\n",
    "        \n",
    "        # NDCG@10\n",
    "        if 'ndcg_at_10' in results_df.columns and pd.notna(results_df.loc[model, 'ndcg_at_10']):\n",
    "            ndcg_norm = (results_df.loc[model, 'ndcg_at_10'] - results_df['ndcg_at_10'].min()) / (results_df['ndcg_at_10'].max() - results_df['ndcg_at_10'].min())\n",
    "            score += ndcg_norm * 0.2\n",
    "            count += 0.2\n",
    "        \n",
    "        # Coverage\n",
    "        if 'coverage' in results_df.columns and pd.notna(results_df.loc[model, 'coverage']):\n",
    "            cov_norm = (results_df.loc[model, 'coverage'] - results_df['coverage'].min()) / (results_df['coverage'].max() - results_df['coverage'].min())\n",
    "            score += cov_norm * 0.2\n",
    "            count += 0.2\n",
    "        \n",
    "        if count > 0:\n",
    "            composite_scores[model] = score / count\n",
    "    \n",
    "    if composite_scores:\n",
    "        sorted_composite = sorted(composite_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        for i, (model, score) in enumerate(sorted_composite, 1):\n",
    "            print(f\"  {i}. {model}: {score:.4f}\")\n",
    "        \n",
    "        best_overall = sorted_composite[0][0]\n",
    "        print(f\"\\n🏆 MEILLEUR MODÈLE GLOBAL: {best_overall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualisations Avancées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisations comparatives\n",
    "if comprehensive_results and not results_df.empty:\n",
    "    print(\"📈 Génération des visualisations...\")\n",
    "    \n",
    "    # Configuration des sous-graphiques\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('Évaluation Comparative des Modèles de Recommandation', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. RMSE Comparison\n",
    "    if 'rmse' in results_df.columns:\n",
    "        rmse_data = results_df['rmse'].dropna()\n",
    "        if not rmse_data.empty:\n",
    "            rmse_data.plot(kind='bar', ax=axes[0,0], color='lightcoral', edgecolor='black')\n",
    "            axes[0,0].set_title('RMSE par Modèle (plus bas = meilleur)')\n",
    "            axes[0,0].set_ylabel('RMSE')\n",
    "            axes[0,0].tick_params(axis='x', rotation=45)\n",
    "            axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Precision@10 Comparison\n",
    "    if 'precision_at_10' in results_df.columns:\n",
    "        prec10_data = results_df['precision_at_10'].dropna()\n",
    "        if not prec10_data.empty:\n",
    "            prec10_data.plot(kind='bar', ax=axes[0,1], color='lightblue', edgecolor='black')\n",
    "            axes[0,1].set_title('Precision@10 par Modèle')\n",
    "            axes[0,1].set_ylabel('Precision@10')\n",
    "            axes[0,1].tick_params(axis='x', rotation=45)\n",
    "            axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. NDCG@10 Comparison\n",
    "    if 'ndcg_at_10' in results_df.columns:\n",
    "        ndcg10_data = results_df['ndcg_at_10'].dropna()\n",
    "        if not ndcg10_data.empty:\n",
    "            ndcg10_data.plot(kind='bar', ax=axes[0,2], color='lightgreen', edgecolor='black')\n",
    "            axes[0,2].set_title('NDCG@10 par Modèle')\n",
    "            axes[0,2].set_ylabel('NDCG@10')\n",
    "            axes[0,2].tick_params(axis='x', rotation=45)\n",
    "            axes[0,2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Coverage Comparison\n",
    "    if 'coverage' in results_df.columns:\n",
    "        coverage_data = results_df['coverage'].dropna()\n",
    "        if not coverage_data.empty:\n",
    "            coverage_data.plot(kind='bar', ax=axes[1,0], color='gold', edgecolor='black')\n",
    "            axes[1,0].set_title('Couverture par Modèle')\n",
    "            axes[1,0].set_ylabel('Couverture')\n",
    "            axes[1,0].tick_params(axis='x', rotation=45)\n",
    "            axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Evaluation Time Comparison\n",
    "    if 'evaluation_time' in results_df.columns:\n",
    "        time_data = results_df['evaluation_time'].dropna()\n",
    "        if not time_data.empty:\n",
    "            time_data.plot(kind='bar', ax=axes[1,1], color='plum', edgecolor='black')\n",
    "            axes[1,1].set_title('Temps d\\'Évaluation par Modèle')\n",
    "            axes[1,1].set_ylabel('Temps (secondes)')\n",
    "            axes[1,1].tick_params(axis='x', rotation=45)\n",
    "            axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Composite Score\n",
    "    if composite_scores:\n",
    "        comp_df = pd.Series(composite_scores)\n",
    "        comp_df.plot(kind='bar', ax=axes[1,2], color='orange', edgecolor='black')\n",
    "        axes[1,2].set_title('Score Composite par Modèle')\n",
    "        axes[1,2].set_ylabel('Score Composite')\n",
    "        axes[1,2].tick_params(axis='x', rotation=45)\n",
    "        axes[1,2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"✅ Visualisations générées\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphique radar multi-métriques\n",
    "if comprehensive_results and not results_df.empty:\n",
    "    try:\n",
    "        # Sélection des métriques pour le radar\n",
    "        radar_metrics = []\n",
    "        if 'rmse' in results_df.columns:\n",
    "            radar_metrics.append('rmse')\n",
    "        if 'precision_at_10' in results_df.columns:\n",
    "            radar_metrics.append('precision_at_10')\n",
    "        if 'ndcg_at_10' in results_df.columns:\n",
    "            radar_metrics.append('ndcg_at_10')\n",
    "        if 'coverage' in results_df.columns:\n",
    "            radar_metrics.append('coverage')\n",
    "        \n",
    "        if len(radar_metrics) >= 3:\n",
    "            # Normalisation des métriques\n",
    "            radar_df = results_df[radar_metrics].dropna()\n",
    "            \n",
    "            if not radar_df.empty:\n",
    "                normalized_radar = radar_df.copy()\n",
    "                \n",
    "                for col in radar_metrics:\n",
    "                    if col == 'rmse':  # Métrique à minimiser\n",
    "                        normalized_radar[col] = 1 - (radar_df[col] - radar_df[col].min()) / (radar_df[col].max() - radar_df[col].min())\n",
    "                    else:  # Métriques à maximiser\n",
    "                        normalized_radar[col] = (radar_df[col] - radar_df[col].min()) / (radar_df[col].max() - radar_df[col].min())\n",
    "                \n",
    "                # Création du graphique radar\n",
    "                fig_radar = go.Figure()\n",
    "                \n",
    "                colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown', 'pink', 'gray']\n",
    "                \n",
    "                for i, model in enumerate(normalized_radar.index):\n",
    "                    values = normalized_radar.loc[model].tolist()\n",
    "                    values.append(values[0])  # Fermer le polygone\n",
    "                    \n",
    "                    theta = radar_metrics + [radar_metrics[0]]\n",
    "                    \n",
    "                    fig_radar.add_trace(go.Scatterpolar(\n",
    "                        r=values,\n",
    "                        theta=theta,\n",
    "                        fill='toself',\n",
    "                        name=model,\n",
    "                        line_color=colors[i % len(colors)]\n",
    "                    ))\n",
    "                \n",
    "                fig_radar.update_layout(\n",
    "                    polar=dict(\n",
    "                        radialaxis=dict(\n",
    "                            visible=True,\n",
    "                            range=[0, 1]\n",
    "                        )),\n",
    "                    showlegend=True,\n",
    "                    title=\"Comparaison Multi-Métriques des Modèles (Normalisé)\",\n",
    "                    width=800,\n",
    "                    height=600\n",
    "                )\n",
    "                \n",
    "                fig_radar.show()\n",
    "                print(\"✅ Graphique radar généré\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyse des Biais et Robustesse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse des biais\n",
    "def analyze_bias(model_manager, model_name, test_data, movies_data):\n",
    "    \"\"\"Analyse des biais dans les recommandations\"\"\"\n",
    "    bias_analysis = {}\n",
    "    \n",
    "    try:\n",
    "        # Échantillon d'utilisateurs\n",
    "        sample_users = test_data['userId'].unique()[:50]\n",
    "        \n",
    "        all_recommendations = []\n",
    "        user_activity_levels = []\n",
    "        \n",
    "        for user_id in sample_users:\n",
    "            # Niveau d'activité de l'utilisateur\n",
    "            user_ratings_count = len(preprocessor.train_data[preprocessor.train_data['userId'] == user_id])\n",
    "            user_activity_levels.append(user_ratings_count)\n",
    "            \n",
    "            try:\n",
    "                if 'collaborative' in model_name:\n",
    "                    recommendations = model_manager.get_recommendations(\n",
    "                        model_name.split('_')[1], user_id, 10\n",
    "                    )\n",
    "                elif 'content' in model_name:\n",
    "                    user_history = preprocessor.train_data[preprocessor.train_data['userId'] == user_id]\n",
    "                    if len(user_history) > 0:\n",
    "                        ref_movie = user_history.iloc[0]['movieId']\n",
    "                        recommendations = model_manager.get_recommendations(\n",
    "                            model_name.split('_')[1], ref_movie, 10\n",
    "                        )\n",
    "                    else:\n",
    "                        continue\n",
    "                elif 'hybrid' in model_name:\n",
    "                    recommendations = model_manager.get_recommendations(\n",
    "                        model_name.split('_')[1], user_id, 10\n",
    "                    )\n",
    "                \n",
    "                recommended_movies = [item[0] for item in recommendations]\n",
    "                all_recommendations.extend(recommended_movies)\n",
    "                \n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        if all_recommendations:\n",
    "            # 1. Biais de popularité\n",
    "            movie_popularity = preprocessor.train_data['movieId'].value_counts()\n",
    "            recommended_popularity = []\n",
    "            \n",
    "            for movie_id in all_recommendations:\n",
    "                if movie_id in movie_popularity.index:\n",
    "                    recommended_popularity.append(movie_popularity[movie_id])\n",
    "                else:\n",
    "                    recommended_popularity.append(0)\n",
    "            \n",
    "            if recommended_popularity:\n",
    "                avg_recommended_popularity = np.mean(recommended_popularity)\n",
    "                avg_overall_popularity = movie_popularity.mean()\n",
    "                bias_analysis['popularity_bias'] = avg_recommended_popularity / avg_overall_popularity\n",
    "            \n",
    "            # 2. Biais temporel (films récents vs anciens)\n",
    "            recommended_years = []\n",
    "            for movie_id in all_recommendations:\n",
    "                movie_info = movies_data[movies_data['movieId'] == movie_id]\n",
    "                if len(movie_info) > 0 and pd.notna(movie_info.iloc[0].get('year')):\n",
    "                    recommended_years.append(movie_info.iloc[0]['year'])\n",
    "            \n",
    "            if recommended_years:\n",
    "                avg_recommended_year = np.mean(recommended_years)\n",
    "                avg_overall_year = movies_data['year'].mean()\n",
    "                bias_analysis['temporal_bias'] = avg_recommended_year - avg_overall_year\n",
    "            \n",
    "            # 3. Concentration des recommandations (Gini coefficient)\n",
    "            recommendation_counts = pd.Series(all_recommendations).value_counts()\n",
    "            if len(recommendation_counts) > 1:\n",
    "                # Calcul simplifié du coefficient de Gini\n",
    "                sorted_counts = np.sort(recommendation_counts.values)\n",
    "                n = len(sorted_counts)\n",
    "                cumsum = np.cumsum(sorted_counts)\n",
    "                gini = (n + 1 - 2 * np.sum(cumsum) / cumsum[-1]) / n\n",
    "                bias_analysis['gini_coefficient'] = gini\n",
    "    \n",
    "    except Exception as e:\n",
    "        bias_analysis['error'] = str(e)\n",
    "    \n",
    "    return bias_analysis\n",
    "\n",
    "print(\"🔍 === ANALYSE DES BIAIS ===")\n",
    "\n",
    "bias_results = {}\n",
    "\n",
    "for model_name, model_manager in models_to_evaluate[:3]:  # Limiter à 3 modèles\n",
    "    print(f\"Analyse des biais pour {model_name}...\")\n",
    "    bias_analysis = analyze_bias(model_manager, model_name, preprocessor.test_data, preprocessor.movies)\n",
    "    bias_results[model_name] = bias_analysis\n",
    "    \n",
    "    if 'error' not in bias_analysis:\n",
    "        print(f\"  Biais de popularité: {bias_analysis.get('popularity_bias', 'N/A'):.3f}\")\n",
    "        print(f\"  Biais temporel: {bias_analysis.get('temporal_bias', 'N/A'):.1f} années\")\n",
    "        print(f\"  Coefficient de Gini: {bias_analysis.get('gini_coefficient', 'N/A'):.3f}\")