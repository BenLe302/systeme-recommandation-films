{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# √âvaluation Approfondie - Syst√®me de Recommandation MovieLens\n",
    "\n",
    "**Auteur:** Dady Akrou Cyrille  \n",
    "**Email:** cyrilledady0501@gmail.com  \n",
    "**Date:** D√©cembre 2024\n",
    "\n",
    "Ce notebook effectue une √©valuation approfondie des mod√®les de recommandation :\n",
    "- M√©triques de pr√©cision (RMSE, MAE)\n",
    "- M√©triques de ranking (Precision@K, Recall@K, NDCG@K)\n",
    "- M√©triques de diversit√© et de couverture\n",
    "- Analyse des biais et de la robustesse\n",
    "- Tests A/B simul√©s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports n√©cessaires\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Ajouter le r√©pertoire src au path\n",
    "sys.path.append(str(Path('../src').resolve()))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "import yaml\n",
    "import json\n",
    "from datetime import datetime\n",
    "import time\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "# Imports des modules personnalis√©s\n",
    "from data_processing.preprocess import MovieLensPreprocessor\n",
    "from models.collaborative_filtering import CollaborativeFilteringManager\n",
    "from models.content_based_filtering import ContentBasedManager\n",
    "from models.hybrid_system import HybridSystemManager\n",
    "from evaluation.metrics import ModelEvaluator, RecommendationMetrics\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Configuration des graphiques\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"‚úÖ Imports r√©alis√©s avec succ√®s!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement de la configuration\n",
    "config_path = Path('../config/config.yaml')\n",
    "\n",
    "with open(config_path, 'r', encoding='utf-8') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Chargement des r√©sultats de mod√©lisation pr√©c√©dents\n",
    "results_path = Path('../data/processed/modeling_results.json')\n",
    "\n",
    "if results_path.exists():\n",
    "    with open(results_path, 'r', encoding='utf-8') as f:\n",
    "        previous_results = json.load(f)\n",
    "    print(\"‚úÖ R√©sultats de mod√©lisation charg√©s\")\n",
    "else:\n",
    "    previous_results = None\n",
    "    print(\"‚ö†Ô∏è Aucun r√©sultat de mod√©lisation trouv√©\")\n",
    "\n",
    "print(f\"Configuration charg√©e: {len(config['evaluation']['metrics'])} m√©triques d'√©valuation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chargement des Donn√©es et Mod√®les"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation du pr√©processeur et chargement des donn√©es\n",
    "print(\"üîÑ Chargement des donn√©es pr√©process√©es...\")\n",
    "\n",
    "preprocessor = MovieLensPreprocessor(config)\n",
    "\n",
    "try:\n",
    "    # Chargement des donn√©es sauvegard√©es\n",
    "    processed_data_path = Path(config['data']['processed_path'])\n",
    "    \n",
    "    if (processed_data_path / 'train_data.csv').exists():\n",
    "        preprocessor.train_data = pd.read_csv(processed_data_path / 'train_data.csv')\n",
    "        preprocessor.val_data = pd.read_csv(processed_data_path / 'val_data.csv')\n",
    "        preprocessor.test_data = pd.read_csv(processed_data_path / 'test_data.csv')\n",
    "        preprocessor.movies = pd.read_csv(processed_data_path / 'movies_processed.csv')\n",
    "        \n",
    "        print(f\"‚úÖ Donn√©es charg√©es:\")\n",
    "        print(f\"   - Train: {len(preprocessor.train_data):,} ratings\")\n",
    "        print(f\"   - Validation: {len(preprocessor.val_data):,} ratings\")\n",
    "        print(f\"   - Test: {len(preprocessor.test_data):,} ratings\")\n",
    "        print(f\"   - Films: {len(preprocessor.movies):,} films\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Donn√©es pr√©process√©es non trouv√©es, rechargement n√©cessaire\")\n",
    "        # Recharger et pr√©processer\n",
    "        preprocessor.load_data()\n",
    "        preprocessor.clean_data()\n",
    "        preprocessor.process_movies()\n",
    "        preprocessor.split_data()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur lors du chargement: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation des gestionnaires de mod√®les\n",
    "print(\"ü§ñ Initialisation des gestionnaires de mod√®les...\")\n",
    "\n",
    "# Gestionnaire collaboratif\n",
    "cf_manager = CollaborativeFilteringManager(config)\n",
    "cf_manager.load_data(\n",
    "    train_data=preprocessor.train_data,\n",
    "    val_data=preprocessor.val_data,\n",
    "    test_data=preprocessor.test_data\n",
    ")\n",
    "\n",
    "# Gestionnaire de contenu\n",
    "content_manager = ContentBasedManager(config)\n",
    "content_manager.load_data(\n",
    "    movies=preprocessor.movies,\n",
    "    ratings=preprocessor.train_data\n",
    ")\n",
    "\n",
    "# Gestionnaire hybride\n",
    "hybrid_manager = HybridSystemManager(config)\n",
    "\n",
    "# √âvaluateur\n",
    "evaluator = ModelEvaluator(config)\n",
    "metrics_calculator = RecommendationMetrics()\n",
    "\n",
    "print(\"‚úÖ Gestionnaires initialis√©s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chargement des Mod√®les Pr√©-entra√Æn√©s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des mod√®les sauvegard√©s\n",
    "print(\"üìÇ Chargement des mod√®les pr√©-entra√Æn√©s...\")\n",
    "\n",
    "models_path = Path(config['data']['models_path'])\n",
    "loaded_models = {}\n",
    "\n",
    "# Mod√®les collaboratifs\n",
    "collaborative_models = ['svd', 'nmf', 'knn']\n",
    "for model_name in collaborative_models:\n",
    "    try:\n",
    "        cf_manager.load_model(model_name)\n",
    "        loaded_models[f'collaborative_{model_name}'] = cf_manager\n",
    "        print(f\"‚úÖ Mod√®le {model_name.upper()} charg√©\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Impossible de charger {model_name}: {e}\")\n",
    "\n",
    "# Mod√®les de contenu\n",
    "content_models = ['tfidf', 'genre_based']\n",
    "for model_name in content_models:\n",
    "    try:\n",
    "        content_manager.load_model(model_name)\n",
    "        loaded_models[f'content_{model_name}'] = content_manager\n",
    "        print(f\"‚úÖ Mod√®le {model_name.upper()} charg√©\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Impossible de charger {model_name}: {e}\")\n",
    "\n",
    "# Mod√®les hybrides\n",
    "hybrid_models = ['weighted', 'switching']\n",
    "for model_name in hybrid_models:\n",
    "    try:\n",
    "        hybrid_manager.load_model(model_name)\n",
    "        loaded_models[f'hybrid_{model_name}'] = hybrid_manager\n",
    "        print(f\"‚úÖ Mod√®le hybride {model_name.upper()} charg√©\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Impossible de charger {model_name}: {e}\")\n",
    "\n",
    "print(f\"\\nüìä Total: {len(loaded_models)} mod√®les charg√©s pour l'√©valuation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. √âvaluation Compl√®te des Mod√®les"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction d'√©valuation compl√®te\n",
    "def comprehensive_evaluation(model_manager, model_name, test_data, movies_data, k_values=[5, 10, 20]):\n",
    "    \"\"\"√âvaluation compl√®te d'un mod√®le\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    print(f\"üîç √âvaluation de {model_name}...\")\n",
    "    \n",
    "    try:\n",
    "        # 1. M√©triques de pr√©cision (pour les mod√®les collaboratifs)\n",
    "        if 'collaborative' in model_name:\n",
    "            precision_metrics = model_manager.evaluate_model(model_name.split('_')[1])\n",
    "            results.update(precision_metrics)\n",
    "        \n",
    "        # 2. M√©triques de ranking\n",
    "        ranking_metrics = {}\n",
    "        \n",
    "        # √âchantillonnage pour acc√©l√©rer l'√©valuation\n",
    "        sample_users = test_data['userId'].unique()[:100]  # 100 utilisateurs\n",
    "        \n",
    "        all_precisions = {k: [] for k in k_values}\n",
    "        all_recalls = {k: [] for k in k_values}\n",
    "        all_ndcgs = {k: [] for k in k_values}\n",
    "        \n",
    "        for user_id in sample_users:\n",
    "            # Films r√©ellement appr√©ci√©s par l'utilisateur (rating >= 4)\n",
    "            user_test_data = test_data[test_data['userId'] == user_id]\n",
    "            relevant_items = set(user_test_data[user_test_data['rating'] >= 4]['movieId'].tolist())\n",
    "            \n",
    "            if len(relevant_items) == 0:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Obtenir les recommandations\n",
    "                if 'collaborative' in model_name:\n",
    "                    recommendations = model_manager.get_recommendations(\n",
    "                        model_name.split('_')[1], user_id, max(k_values)\n",
    "                    )\n",
    "                elif 'content' in model_name:\n",
    "                    # Pour les mod√®les de contenu, utiliser un film de l'historique\n",
    "                    user_history = preprocessor.train_data[preprocessor.train_data['userId'] == user_id]\n",
    "                    if len(user_history) > 0:\n",
    "                        ref_movie = user_history.iloc[0]['movieId']\n",
    "                        recommendations = model_manager.get_recommendations(\n",
    "                            model_name.split('_')[1], ref_movie, max(k_values)\n",
    "                        )\n",
    "                    else:\n",
    "                        continue\n",
    "                elif 'hybrid' in model_name:\n",
    "                    recommendations = model_manager.get_recommendations(\n",
    "                        model_name.split('_')[1], user_id, max(k_values)\n",
    "                    )\n",
    "                \n",
    "                recommended_items = [item[0] for item in recommendations]\n",
    "                \n",
    "                # Calculer les m√©triques pour chaque k\n",
    "                for k in k_values:\n",
    "                    top_k_items = set(recommended_items[:k])\n",
    "                    \n",
    "                    # Precision@k\n",
    "                    precision_k = len(top_k_items.intersection(relevant_items)) / k\n",
    "                    all_precisions[k].append(precision_k)\n",
    "                    \n",
    "                    # Recall@k\n",
    "                    recall_k = len(top_k_items.intersection(relevant_items)) / len(relevant_items)\n",
    "                    all_recalls[k].append(recall_k)\n",
    "                    \n",
    "                    # NDCG@k (simplifi√©)\n",
    "                    dcg = 0\n",
    "                    for i, item in enumerate(recommended_items[:k]):\n",
    "                        if item in relevant_items:\n",
    "                            dcg += 1 / np.log2(i + 2)\n",
    "                    \n",
    "                    # IDCG (ideal DCG)\n",
    "                    idcg = sum(1 / np.log2(i + 2) for i in range(min(k, len(relevant_items))))\n",
    "                    \n",
    "                    ndcg_k = dcg / idcg if idcg > 0 else 0\n",
    "                    all_ndcgs[k].append(ndcg_k)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        # Moyennes des m√©triques\n",
    "        for k in k_values:\n",
    "            if all_precisions[k]:\n",
    "                ranking_metrics[f'precision_at_{k}'] = np.mean(all_precisions[k])\n",
    "                ranking_metrics[f'recall_at_{k}'] = np.mean(all_recalls[k])\n",
    "                ranking_metrics[f'ndcg_at_{k}'] = np.mean(all_ndcgs[k])\n",
    "                ranking_metrics[f'f1_at_{k}'] = 2 * ranking_metrics[f'precision_at_{k}'] * ranking_metrics[f'recall_at_{k}'] / (ranking_metrics[f'precision_at_{k}'] + ranking_metrics[f'recall_at_{k}']) if (ranking_metrics[f'precision_at_{k}'] + ranking_metrics[f'recall_at_{k}']) > 0 else 0\n",
    "        \n",
    "        results.update(ranking_metrics)\n",
    "        \n",
    "        # 3. M√©triques de diversit√©\n",
    "        diversity_metrics = calculate_diversity_metrics(model_manager, model_name, sample_users[:20], movies_data)\n",
    "        results.update(diversity_metrics)\n",
    "        \n",
    "        print(f\"‚úÖ {model_name} √©valu√©\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur lors de l'√©valuation de {model_name}: {e}\")\n",
    "        results['error'] = str(e)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def calculate_diversity_metrics(model_manager, model_name, sample_users, movies_data):\n",
    "    \"\"\"Calcul des m√©triques de diversit√©\"\"\"\n",
    "    diversity_metrics = {}\n",
    "    \n",
    "    try:\n",
    "        all_recommendations = []\n",
    "        all_genres = []\n",
    "        \n",
    "        for user_id in sample_users:\n",
    "            try:\n",
    "                if 'collaborative' in model_name:\n",
    "                    recommendations = model_manager.get_recommendations(\n",
    "                        model_name.split('_')[1], user_id, 10\n",
    "                    )\n",
    "                elif 'content' in model_name:\n",
    "                    user_history = preprocessor.train_data[preprocessor.train_data['userId'] == user_id]\n",
    "                    if len(user_history) > 0:\n",
    "                        ref_movie = user_history.iloc[0]['movieId']\n",
    "                        recommendations = model_manager.get_recommendations(\n",
    "                            model_name.split('_')[1], ref_movie, 10\n",
    "                        )\n",
    "                    else:\n",
    "                        continue\n",
    "                elif 'hybrid' in model_name:\n",
    "                    recommendations = model_manager.get_recommendations(\n",
    "                        model_name.split('_')[1], user_id, 10\n",
    "                    )\n",
    "                \n",
    "                recommended_movies = [item[0] for item in recommendations]\n",
    "                all_recommendations.extend(recommended_movies)\n",
    "                \n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        if all_recommendations:\n",
    "            # Coverage: pourcentage de films uniques recommand√©s\n",
    "            unique_recommendations = len(set(all_recommendations))\n",
    "            total_movies = len(movies_data)\n",
    "            diversity_metrics['coverage'] = unique_recommendations / total_movies\n",
    "            \n",
    "            # Diversit√© des genres\n",
    "            if all_genres:\n",
    "                unique_genres = len(set(all_genres))\n",
    "                total_genres = len(set([genre for genres in movies_data['genres'].dropna() for genre in genres.split('|')]))\n",
    "                diversity_metrics['genre_diversity'] = unique_genres / total_genres\n",
    "                \n",
    "                # Entropie des genres\n",
    "                genre_counts = pd.Series(all_genres).value_counts()\n",
    "                genre_probs = genre_counts / genre_counts.sum()\n",
    "                diversity_metrics['genre_entropy'] = -sum(p * np.log2(p) for p in genre_probs if p > 0)\n",
    "        \n",
    "    except Exception as e:\n",
    "        diversity_metrics['diversity_error'] = str(e)\n",
    "    \n",
    "    return diversity_metrics\n",
    "\n",
    "print(\"‚úÖ Fonctions d'√©valuation d√©finies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âvaluation compl√®te de tous les mod√®les\n",
    "print(\"üöÄ === √âVALUATION COMPL√àTE DES MOD√àLES ===")\n",
    "\n",
    "comprehensive_results = {}\n",
    "evaluation_start_time = time.time()\n",
    "\n",
    "# Liste des mod√®les √† √©valuer\n",
    "models_to_evaluate = []\n",
    "\n",
    "# Ajouter les mod√®les collaboratifs\n",
    "for model in collaborative_models:\n",
    "    if f'collaborative_{model}' in loaded_models:\n",
    "        models_to_evaluate.append((f'collaborative_{model}', cf_manager))\n",
    "\n",
    "# Ajouter les mod√®les de contenu\n",
    "for model in content_models:\n",
    "    if f'content_{model}' in loaded_models:\n",
    "        models_to_evaluate.append((f'content_{model}', content_manager))\n",
    "\n",
    "# Ajouter les mod√®les hybrides\n",
    "for model in hybrid_models:\n",
    "    if f'hybrid_{model}' in loaded_models:\n",
    "        models_to_evaluate.append((f'hybrid_{model}', hybrid_manager))\n",
    "\n",
    "print(f\"üìä {len(models_to_evaluate)} mod√®les √† √©valuer\")\n",
    "\n",
    "# √âvaluation de chaque mod√®le\n",
    "for model_name, model_manager in models_to_evaluate:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    results = comprehensive_evaluation(\n",
    "        model_manager, \n",
    "        model_name, \n",
    "        preprocessor.test_data, \n",
    "        preprocessor.movies\n",
    "    )\n",
    "    \n",
    "    evaluation_time = time.time() - start_time\n",
    "    results['evaluation_time'] = evaluation_time\n",
    "    \n",
    "    comprehensive_results[model_name] = results\n",
    "    \n",
    "    print(f\"‚è±Ô∏è {model_name} √©valu√© en {evaluation_time:.2f}s\")\n",
    "\n",
    "total_evaluation_time = time.time() - evaluation_start_time\n",
    "print(f\"\\n‚úÖ √âvaluation compl√®te termin√©e en {total_evaluation_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyse Comparative D√©taill√©e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©ation du DataFrame de r√©sultats\n",
    "if comprehensive_results:\n",
    "    results_df = pd.DataFrame(comprehensive_results).T\n",
    "    \n",
    "    print(\"üìä === R√âSULTATS COMPARATIFS D√âTAILL√âS ===")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Affichage des m√©triques principales\n",
    "    print(\"\\nüéØ M√âTRIQUES DE PR√âCISION:\")\n",
    "    precision_cols = [col for col in results_df.columns if 'rmse' in col or 'mae' in col]\n",
    "    if precision_cols:\n",
    "        precision_df = results_df[precision_cols].dropna()\n",
    "        if not precision_df.empty:\n",
    "            display(precision_df.round(4))\n",
    "    \n",
    "    print(\"\\nüéØ M√âTRIQUES DE RANKING (K=10):\")\n",
    "    ranking_cols = [col for col in results_df.columns if '_at_10' in col]\n",
    "    if ranking_cols:\n",
    "        ranking_df = results_df[ranking_cols].dropna()\n",
    "        if not ranking_df.empty:\n",
    "            display(ranking_df.round(4))\n",
    "    \n",
    "    print(\"\\nüéØ M√âTRIQUES DE DIVERSIT√â:\")\n",
    "    diversity_cols = [col for col in results_df.columns if 'coverage' in col or 'diversity' in col or 'entropy' in col]\n",
    "    if diversity_cols:\n",
    "        diversity_df = results_df[diversity_cols].dropna()\n",
    "        if not diversity_df.empty:\n",
    "            display(diversity_df.round(4))\n",
    "    \n",
    "    # Tableau complet\n",
    "    print(\"\\nüìã TABLEAU COMPLET:\")\n",
    "    display(results_df.round(4))\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Aucun r√©sultat d'√©valuation disponible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identification des meilleurs mod√®les par cat√©gorie\n",
    "if comprehensive_results and not results_df.empty:\n",
    "    print(\"üèÜ === CLASSEMENT DES MOD√àLES ===")\n",
    "    print()\n",
    "    \n",
    "    best_models = {}\n",
    "    \n",
    "    # Meilleur pour la pr√©cision (RMSE)\n",
    "    if 'rmse' in results_df.columns:\n",
    "        rmse_data = results_df['rmse'].dropna()\n",
    "        if not rmse_data.empty:\n",
    "            best_rmse = rmse_data.idxmin()\n",
    "            best_models['precision_rmse'] = (best_rmse, rmse_data[best_rmse])\n",
    "            print(f\"ü•á MEILLEUR RMSE: {best_rmse} ({rmse_data[best_rmse]:.4f})\")\n",
    "    \n",
    "    # Meilleur pour MAE\n",
    "    if 'mae' in results_df.columns:\n",
    "        mae_data = results_df['mae'].dropna()\n",
    "        if not mae_data.empty:\n",
    "            best_mae = mae_data.idxmin()\n",
    "            best_models['precision_mae'] = (best_mae, mae_data[best_mae])\n",
    "            print(f\"ü•á MEILLEUR MAE: {best_mae} ({mae_data[best_mae]:.4f})\")\n",
    "    \n",
    "    # Meilleur pour Precision@10\n",
    "    if 'precision_at_10' in results_df.columns:\n",
    "        prec10_data = results_df['precision_at_10'].dropna()\n",
    "        if not prec10_data.empty:\n",
    "            best_prec10 = prec10_data.idxmax()\n",
    "            best_models['ranking_precision'] = (best_prec10, prec10_data[best_prec10])\n",
    "            print(f\"ü•á MEILLEUR PRECISION@10: {best_prec10} ({prec10_data[best_prec10]:.4f})\")\n",
    "    \n",
    "    # Meilleur pour NDCG@10\n",
    "    if 'ndcg_at_10' in results_df.columns:\n",
    "        ndcg10_data = results_df['ndcg_at_10'].dropna()\n",
    "        if not ndcg10_data.empty:\n",
    "            best_ndcg10 = ndcg10_data.idxmax()\n",
    "            best_models['ranking_ndcg'] = (best_ndcg10, ndcg10_data[best_ndcg10])\n",
    "            print(f\"ü•á MEILLEUR NDCG@10: {best_ndcg10} ({ndcg10_data[best_ndcg10]:.4f})\")\n",
    "    \n",
    "    # Meilleur pour la diversit√©\n",
    "    if 'coverage' in results_df.columns:\n",
    "        coverage_data = results_df['coverage'].dropna()\n",
    "        if not coverage_data.empty:\n",
    "            best_coverage = coverage_data.idxmax()\n",
    "            best_models['diversity_coverage'] = (best_coverage, coverage_data[best_coverage])\n",
    "            print(f\"ü•á MEILLEURE COUVERTURE: {best_coverage} ({coverage_data[best_coverage]:.4f})\")\n",
    "    \n",
    "    # Temps d'√©valuation\n",
    "    if 'evaluation_time' in results_df.columns:\n",
    "        time_data = results_df['evaluation_time'].dropna()\n",
    "        if not time_data.empty:\n",
    "            fastest_model = time_data.idxmin()\n",
    "            print(f\"‚ö° PLUS RAPIDE: {fastest_model} ({time_data[fastest_model]:.2f}s)\")\n",
    "    \n",
    "    # Score composite (moyenne normalis√©e)\n",
    "    print(\"\\nüéØ SCORE COMPOSITE (moyenne normalis√©e):\")\n",
    "    composite_scores = {}\n",
    "    \n",
    "    for model in results_df.index:\n",
    "        score = 0\n",
    "        count = 0\n",
    "        \n",
    "        # RMSE (normalis√© invers√©)\n",
    "        if 'rmse' in results_df.columns and pd.notna(results_df.loc[model, 'rmse']):\n",
    "            rmse_norm = 1 - (results_df.loc[model, 'rmse'] - results_df['rmse'].min()) / (results_df['rmse'].max() - results_df['rmse'].min())\n",
    "            score += rmse_norm * 0.3\n",
    "            count += 0.3\n",
    "        \n",
    "        # Precision@10\n",
    "        if 'precision_at_10' in results_df.columns and pd.notna(results_df.loc[model, 'precision_at_10']):\n",
    "            prec_norm = (results_df.loc[model, 'precision_at_10'] - results_df['precision_at_10'].min()) / (results_df['precision_at_10'].max() - results_df['precision_at_10'].min())\n",
    "            score += prec_norm * 0.3\n",
    "            count += 0.3\n",
    "        \n",
    "        # NDCG@10\n",
    "        if 'ndcg_at_10' in results_df.columns and pd.notna(results_df.loc[model, 'ndcg_at_10']):\n",
    "            ndcg_norm = (results_df.loc[model, 'ndcg_at_10'] - results_df['ndcg_at_10'].min()) / (results_df['ndcg_at_10'].max() - results_df['ndcg_at_10'].min())\n",
    "            score += ndcg_norm * 0.2\n",
    "            count += 0.2\n",
    "        \n",
    "        # Coverage\n",
    "        if 'coverage' in results_df.columns and pd.notna(results_df.loc[model, 'coverage']):\n",
    "            cov_norm = (results_df.loc[model, 'coverage'] - results_df['coverage'].min()) / (results_df['coverage'].max() - results_df['coverage'].min())\n",
    "            score += cov_norm * 0.2\n",
    "            count += 0.2\n",
    "        \n",
    "        if count > 0:\n",
    "            composite_scores[model] = score / count\n",
    "    \n",
    "    if composite_scores:\n",
    "        sorted_composite = sorted(composite_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        for i, (model, score) in enumerate(sorted_composite, 1):\n",
    "            print(f\"  {i}. {model}: {score:.4f}\")\n",
    "        \n",
    "        best_overall = sorted_composite[0][0]\n",
    "        print(f\"\\nüèÜ MEILLEUR MOD√àLE GLOBAL: {best_overall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualisations Avanc√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisations comparatives\n",
    "if comprehensive_results and not results_df.empty:\n",
    "    print(\"üìà G√©n√©ration des visualisations...\")\n",
    "    \n",
    "    # Configuration des sous-graphiques\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('√âvaluation Comparative des Mod√®les de Recommandation', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. RMSE Comparison\n",
    "    if 'rmse' in results_df.columns:\n",
    "        rmse_data = results_df['rmse'].dropna()\n",
    "        if not rmse_data.empty:\n",
    "            rmse_data.plot(kind='bar', ax=axes[0,0], color='lightcoral', edgecolor='black')\n",
    "            axes[0,0].set_title('RMSE par Mod√®le (plus bas = meilleur)')\n",
    "            axes[0,0].set_ylabel('RMSE')\n",
    "            axes[0,0].tick_params(axis='x', rotation=45)\n",
    "            axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Precision@10 Comparison\n",
    "    if 'precision_at_10' in results_df.columns:\n",
    "        prec10_data = results_df['precision_at_10'].dropna()\n",
    "        if not prec10_data.empty:\n",
    "            prec10_data.plot(kind='bar', ax=axes[0,1], color='lightblue', edgecolor='black')\n",
    "            axes[0,1].set_title('Precision@10 par Mod√®le')\n",
    "            axes[0,1].set_ylabel('Precision@10')\n",
    "            axes[0,1].tick_params(axis='x', rotation=45)\n",
    "            axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. NDCG@10 Comparison\n",
    "    if 'ndcg_at_10' in results_df.columns:\n",
    "        ndcg10_data = results_df['ndcg_at_10'].dropna()\n",
    "        if not ndcg10_data.empty:\n",
    "            ndcg10_data.plot(kind='bar', ax=axes[0,2], color='lightgreen', edgecolor='black')\n",
    "            axes[0,2].set_title('NDCG@10 par Mod√®le')\n",
    "            axes[0,2].set_ylabel('NDCG@10')\n",
    "            axes[0,2].tick_params(axis='x', rotation=45)\n",
    "            axes[0,2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Coverage Comparison\n",
    "    if 'coverage' in results_df.columns:\n",
    "        coverage_data = results_df['coverage'].dropna()\n",
    "        if not coverage_data.empty:\n",
    "            coverage_data.plot(kind='bar', ax=axes[1,0], color='gold', edgecolor='black')\n",
    "            axes[1,0].set_title('Couverture par Mod√®le')\n",
    "            axes[1,0].set_ylabel('Couverture')\n",
    "            axes[1,0].tick_params(axis='x', rotation=45)\n",
    "            axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Evaluation Time Comparison\n",
    "    if 'evaluation_time' in results_df.columns:\n",
    "        time_data = results_df['evaluation_time'].dropna()\n",
    "        if not time_data.empty:\n",
    "            time_data.plot(kind='bar', ax=axes[1,1], color='plum', edgecolor='black')\n",
    "            axes[1,1].set_title('Temps d\\'√âvaluation par Mod√®le')\n",
    "            axes[1,1].set_ylabel('Temps (secondes)')\n",
    "            axes[1,1].tick_params(axis='x', rotation=45)\n",
    "            axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Composite Score\n",
    "    if composite_scores:\n",
    "        comp_df = pd.Series(composite_scores)\n",
    "        comp_df.plot(kind='bar', ax=axes[1,2], color='orange', edgecolor='black')\n",
    "        axes[1,2].set_title('Score Composite par Mod√®le')\n",
    "        axes[1,2].set_ylabel('Score Composite')\n",
    "        axes[1,2].tick_params(axis='x', rotation=45)\n",
    "        axes[1,2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Visualisations g√©n√©r√©es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphique radar multi-m√©triques\n",
    "if comprehensive_results and not results_df.empty:\n",
    "    try:\n",
    "        # S√©lection des m√©triques pour le radar\n",
    "        radar_metrics = []\n",
    "        if 'rmse' in results_df.columns:\n",
    "            radar_metrics.append('rmse')\n",
    "        if 'precision_at_10' in results_df.columns:\n",
    "            radar_metrics.append('precision_at_10')\n",
    "        if 'ndcg_at_10' in results_df.columns:\n",
    "            radar_metrics.append('ndcg_at_10')\n",
    "        if 'coverage' in results_df.columns:\n",
    "            radar_metrics.append('coverage')\n",
    "        \n",
    "        if len(radar_metrics) >= 3:\n",
    "            # Normalisation des m√©triques\n",
    "            radar_df = results_df[radar_metrics].dropna()\n",
    "            \n",
    "            if not radar_df.empty:\n",
    "                normalized_radar = radar_df.copy()\n",
    "                \n",
    "                for col in radar_metrics:\n",
    "                    if col == 'rmse':  # M√©trique √† minimiser\n",
    "                        normalized_radar[col] = 1 - (radar_df[col] - radar_df[col].min()) / (radar_df[col].max() - radar_df[col].min())\n",
    "                    else:  # M√©triques √† maximiser\n",
    "                        normalized_radar[col] = (radar_df[col] - radar_df[col].min()) / (radar_df[col].max() - radar_df[col].min())\n",
    "                \n",
    "                # Cr√©ation du graphique radar\n",
    "                fig_radar = go.Figure()\n",
    "                \n",
    "                colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown', 'pink', 'gray']\n",
    "                \n",
    "                for i, model in enumerate(normalized_radar.index):\n",
    "                    values = normalized_radar.loc[model].tolist()\n",
    "                    values.append(values[0])  # Fermer le polygone\n",
    "                    \n",
    "                    theta = radar_metrics + [radar_metrics[0]]\n",
    "                    \n",
    "                    fig_radar.add_trace(go.Scatterpolar(\n",
    "                        r=values,\n",
    "                        theta=theta,\n",
    "                        fill='toself',\n",
    "                        name=model,\n",
    "                        line_color=colors[i % len(colors)]\n",
    "                    ))\n",
    "                \n",
    "                fig_radar.update_layout(\n",
    "                    polar=dict(\n",
    "                        radialaxis=dict(\n",
    "                            visible=True,\n",
    "                            range=[0, 1]\n",
    "                        )),\n",
    "                    showlegend=True,\n",
    "                    title=\"Comparaison Multi-M√©triques des Mod√®les (Normalis√©)\",\n",
    "                    width=800,\n",
    "                    height=600\n",
    "                )\n",
    "                \n",
    "                fig_radar.show()\n",
    "                print(\"‚úÖ Graphique radar g√©n√©r√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyse des Biais et Robustesse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse des biais\n",
    "def analyze_bias(model_manager, model_name, test_data, movies_data):\n",
    "    \"\"\"Analyse des biais dans les recommandations\"\"\"\n",
    "    bias_analysis = {}\n",
    "    \n",
    "    try:\n",
    "        # √âchantillon d'utilisateurs\n",
    "        sample_users = test_data['userId'].unique()[:50]\n",
    "        \n",
    "        all_recommendations = []\n",
    "        user_activity_levels = []\n",
    "        \n",
    "        for user_id in sample_users:\n",
    "            # Niveau d'activit√© de l'utilisateur\n",
    "            user_ratings_count = len(preprocessor.train_data[preprocessor.train_data['userId'] == user_id])\n",
    "            user_activity_levels.append(user_ratings_count)\n",
    "            \n",
    "            try:\n",
    "                if 'collaborative' in model_name:\n",
    "                    recommendations = model_manager.get_recommendations(\n",
    "                        model_name.split('_')[1], user_id, 10\n",
    "                    )\n",
    "                elif 'content' in model_name:\n",
    "                    user_history = preprocessor.train_data[preprocessor.train_data['userId'] == user_id]\n",
    "                    if len(user_history) > 0:\n",
    "                        ref_movie = user_history.iloc[0]['movieId']\n",
    "                        recommendations = model_manager.get_recommendations(\n",
    "                            model_name.split('_')[1], ref_movie, 10\n",
    "                        )\n",
    "                    else:\n",
    "                        continue\n",
    "                elif 'hybrid' in model_name:\n",
    "                    recommendations = model_manager.get_recommendations(\n",
    "                        model_name.split('_')[1], user_id, 10\n",
    "                    )\n",
    "                \n",
    "                recommended_movies = [item[0] for item in recommendations]\n",
    "                all_recommendations.extend(recommended_movies)\n",
    "                \n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        if all_recommendations:\n",
    "            # 1. Biais de popularit√©\n",
    "            movie_popularity = preprocessor.train_data['movieId'].value_counts()\n",
    "            recommended_popularity = []\n",
    "            \n",
    "            for movie_id in all_recommendations:\n",
    "                if movie_id in movie_popularity.index:\n",
    "                    recommended_popularity.append(movie_popularity[movie_id])\n",
    "                else:\n",
    "                    recommended_popularity.append(0)\n",
    "            \n",
    "            if recommended_popularity:\n",
    "                avg_recommended_popularity = np.mean(recommended_popularity)\n",
    "                avg_overall_popularity = movie_popularity.mean()\n",
    "                bias_analysis['popularity_bias'] = avg_recommended_popularity / avg_overall_popularity\n",
    "            \n",
    "            # 2. Biais temporel (films r√©cents vs anciens)\n",
    "            recommended_years = []\n",
    "            for movie_id in all_recommendations:\n",
    "                movie_info = movies_data[movies_data['movieId'] == movie_id]\n",
    "                if len(movie_info) > 0 and pd.notna(movie_info.iloc[0].get('year')):\n",
    "                    recommended_years.append(movie_info.iloc[0]['year'])\n",
    "            \n",
    "            if recommended_years:\n",
    "                avg_recommended_year = np.mean(recommended_years)\n",
    "                avg_overall_year = movies_data['year'].mean()\n",
    "                bias_analysis['temporal_bias'] = avg_recommended_year - avg_overall_year\n",
    "            \n",
    "            # 3. Concentration des recommandations (Gini coefficient)\n",
    "            recommendation_counts = pd.Series(all_recommendations).value_counts()\n",
    "            if len(recommendation_counts) > 1:\n",
    "                # Calcul simplifi√© du coefficient de Gini\n",
    "                sorted_counts = np.sort(recommendation_counts.values)\n",
    "                n = len(sorted_counts)\n",
    "                cumsum = np.cumsum(sorted_counts)\n",
    "                gini = (n + 1 - 2 * np.sum(cumsum) / cumsum[-1]) / n\n",
    "                bias_analysis['gini_coefficient'] = gini\n",
    "    \n",
    "    except Exception as e:\n",
    "        bias_analysis['error'] = str(e)\n",
    "    \n",
    "    return bias_analysis\n",
    "\n",
    "print(\"üîç === ANALYSE DES BIAIS ===")\n",
    "\n",
    "bias_results = {}\n",
    "\n",
    "for model_name, model_manager in models_to_evaluate[:3]:  # Limiter √† 3 mod√®les\n",
    "    print(f\"Analyse des biais pour {model_name}...\")\n",
    "    bias_analysis = analyze_bias(model_manager, model_name, preprocessor.test_data, preprocessor.movies)\n",
    "    bias_results[model_name] = bias_analysis\n",
    "    \n",
    "    if 'error' not in bias_analysis:\n",
    "        print(f\"  Biais de popularit√©: {bias_analysis.get('popularity_bias', 'N/A'):.3f}\")\n",
    "        print(f\"  Biais temporel: {bias_analysis.get('temporal_bias', 'N/A'):.1f} ann√©es\")\n",
    "        print(f\"  Coefficient de Gini: {bias_analysis.get('gini_coefficient', 'N/A'):.3f}\")